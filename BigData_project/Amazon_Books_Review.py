# -*- coding: utf-8 -*-
"""686project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OTiF3EaC5vRikK-fQuJDl0D5m9-N0Bqp
"""

# -- coding: utf-8 --
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16YKjhJ72yOa4n8hn7QXPUdMIhNxHZ52P
"""

import pandas as pd


!pip install pyspark

# Load the datasets
rating = pd.read_csv(r"/content/drive/MyDrive/project ITC686/Books_rating.csv")
data = pd.read_csv(r"/content/drive/MyDrive/project ITC686/books_data.csv")

print("  columns of the 'rating' dataset:")
print(rating.columns)

print(" columns of the 'data' dataset:")
print(data.columns)

print("Shape of 'rating' dataset:", rating.shape)
print("Shape of 'data' dataset:", data.shape)

# Merge the datasets on the 'Title' column
books = pd.merge(rating, data, on='Title', how='inner')

# Display the first few rows of the merged dataset
print("Merged Dataset Head:")
print(books.head(2))
print(books.columns)

# Import PySpark and initialize SparkSession
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Create a SparkSession
spark = SparkSession.builder \
    .appName("DataCleaning") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# Extracting useful columns
df = books[['Title','review/score','review/text','authors','categories','ratingsCount','Price']]

# Removing brackets and colons from authors name
df['authors'] = df['authors'].str.extract(r'\'(.*)\'').fillna('Unknown')

# Removing brackets and colons from categories
df['categories'] = df['categories'].str.extract(r'\'(.*)\'').fillna('Unknown')

# Handling missing values in 'review/text'
df['review/text'] = df['review/text'].fillna('')

# Counting the length of each review
df['word_count'] = df['review/text'].apply(lambda x: len(x.split(' ')))

df.head()

# Step 1: Identify Missing Values
print("Missing Values:")
print(df.isnull().sum())

# Step 2: Handle Missing Values
# For simplicity, let's drop rows with any missing values
data_cleaned = df.dropna()

# Step 3: Identify Duplicate Rows
duplicate_rows = data_cleaned[data_cleaned.duplicated()]
print("\nDuplicate Rows:")
print(duplicate_rows)

# Step 4: Drop Duplicate Rows
data_cleaned = data_cleaned.drop_duplicates()

# Summary
print("\nSummary after cleaning:")
print("Number of rows before cleaning:", len(df))
print("Number of rows after cleaning:", len(data_cleaned))
print("First five rows before cleanup:")
print(df.head(5))
print("\nFirst five rows after cleanup:")
print(data_cleaned.head(5))
print(data_cleaned.columns)

# Convert the pandas DataFrame to a PySpark DataFrame
spark_df = spark.createDataFrame(data_cleaned)

# Save the cleaned DataFrame to a CSV file
output_path = "/content/drive/MyDrive/project ITC686/cleaned_merged.csv"
spark_df.write.csv(output_path, header=True, mode="overwrite")

# Stop the SparkSession
spark.stop()

# Import PySpark and initialize SparkSession
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Create a SparkSession
spark = SparkSession.builder \
    .appName("BookAnalysis") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# Load the cleaned and merged DataFrame from the CSV file
file_path = "/content/drive/MyDrive/project ITC686/cleaned_merged.csv"
data_cleaned = spark.read.csv(file_path, header=True, inferSchema=True)

# Create temporary view for the DataFrame
data_cleaned.createOrReplaceTempView("data_cleaned")

# Simple Queries
# Query 1: List the top 10 most reviewed books
query1 = """
SELECT Title, COUNT(*) AS review_count
FROM data_cleaned
GROUP BY Title
ORDER BY review_count DESC
LIMIT 10
"""
result1 = spark.sql(query1)
result1.show()

query2_with_coalesce = """
SELECT authors, COALESCE(AVG(`review/score`), -1) AS avg_review_score
FROM data_cleaned
GROUP BY authors
ORDER BY authors , avg_review_score DESC
"""

result2_with_coalesce = spark.sql(query2_with_coalesce)
result2_with_coalesce.show()

# Query with COALESCE to avoid NULL values
query3_with_coalesce = """
SELECT categories, COALESCE(AVG(`review/score`), -1) AS avg_rating
FROM data_cleaned
GROUP BY categories
ORDER BY avg_rating DESC
LIMIT 5
"""

result3_with_coalesce = spark.sql(query3_with_coalesce)
result3_with_coalesce.show()

# Moderately Complex Queries
# Query 4: Calculate the average word count of reviews for each category
query4 = """
SELECT categories, AVG(word_count) AS avg_word_count
FROM data_cleaned
GROUP BY categories
ORDER BY avg_word_count DESC
"""
result4 = spark.sql(query4)
result4.show()

# Query 5: Determine the total number of ratings for each book
query5 = """
SELECT Title, SUM(ratingsCount) AS total_ratings
FROM data_cleaned
GROUP BY Title
ORDER BY total_ratings DESC
"""
result5 = spark.sql(query5)
result5.show()

# Query to find the authors with the highest number of books, excluding "Unknown"
query6_corrected = """
SELECT authors, COUNT(*) AS book_count
FROM data_cleaned
WHERE authors != 'Unknown'
GROUP BY authors
ORDER BY book_count DESC
LIMIT 5
"""

result6_corrected = spark.sql(query6_corrected)
result6_corrected.show()

# Complex Queries
# Query 7: Perform sentiment analysis on review text to categorize reviews as positive, neutral, or negative
query7 = """
SELECT Title,
    CASE
        WHEN LOWER(`review/text`) LIKE '%good%' OR LOWER(`review/text`) LIKE '%excellent%' THEN 'Positive'
        WHEN LOWER(`review/text`) LIKE '%bad%' OR LOWER(`review/text`) LIKE '%poor%' THEN 'Negative'
        ELSE 'Neutral'
    END AS sentiment
FROM data_cleaned
"""
result7 = spark.sql(query7)
result7.show()

# Query 8: Build a recommendation system to suggest similar books based on categories and ratings
query8 = """
SELECT a.Title AS book_title, b.Title AS recommended_book
FROM data_cleaned a
JOIN (
    SELECT Title, categories
    FROM data_cleaned
    GROUP BY Title, categories
) b
ON a.categories = b.categories AND a.Title != b.Title
"""
result8 = spark.sql(query8)
result8.show()


# Query to find the average review score and word count for each author, sorted by avg_word_count in descending order
query9_corrected = """
SELECT authors,
    AVG(`review/score`) AS avg_review_score,
    AVG(`word_count`) AS avg_word_count
FROM data_cleaned
GROUP BY authors
ORDER BY avg_word_count DESC
"""

# Execute the corrected query and display the results
result9_corrected = spark.sql(query9_corrected)
result9_corrected.show()



# Stop the SparkSession
spark.stop()

#EDA

# Install PySpark
!pip install pyspark

# Import necessary libraries
from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
import seaborn as sns

# Create a SparkSession
spark = SparkSession.builder \
    .appName("BookAnalysis") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# Load the cleaned DataFrame from the CSV file
file_path = "/content/drive/MyDrive/project ITC686/cleaned_merged.csv"
data_cleaned = spark.read.csv(file_path, header=True, inferSchema=True)


# Distribution of Books Based on Genre (Bar Graph)
plt.figure(figsize=(14, 12))
labels = ['Fiction', 'Juvenile Fiction', 'Biography & Autobiography', 'Religion', 'History', 'Business & Economics', 'Computers', 'Cooking', 'Social Science', 'Family & Relationships']
plt.subplot(2, 2, 1)
genre_counts = data_cleaned.groupBy('categories').count().orderBy('count', ascending=False).limit(10).toPandas()
plt.bar(labels, genre_counts['count'], color='skyblue')
plt.title('Distribution of Books Based on Genre', fontsize=15)
plt.xlabel('Genre')
plt.ylabel('Number of Books')
plt.xticks(rotation=45, ha='right')

# Ratings Count Distribution (Histogram)
plt.subplot(2, 2, 2)
ratings_count = data_cleaned.select('ratingsCount').rdd.flatMap(lambda x: x).collect()
sns.histplot(ratings_count, bins=20, kde=True, color='green')
plt.title('Ratings Count Distribution', fontsize=15)
plt.xlabel('Ratings Count')
plt.ylabel('Frequency')
plt.figure(figsize=(10, 8))

# Filter highly rated books with over 4000 ratings
highly_rated_books = data_cleaned.filter(data_cleaned['ratingsCount'] > 4000).select('Title', 'ratingsCount').distinct()

# Convert 'ratingsCount' to numeric
highly_rated_books = highly_rated_books.withColumn('ratingsCount', highly_rated_books['ratingsCount'].cast('integer'))

# Convert Spark DataFrame to Pandas DataFrame for plotting
highly_rated_books_pandas = highly_rated_books.toPandas()

# Plotting the pie chart
plt.figure(figsize=(10, 8))  # Adjust the figsize here
plt.pie(highly_rated_books_pandas['ratingsCount'], labels=highly_rated_books_pandas['Title'], autopct='%1.1f%%', startangle=140)
plt.title('Highest Rated Books with Over 4000 Ratings Each', fontsize=15)
plt.show()

# Scatter plot for ratingsCount vs review_score
plt.figure(figsize=(10, 8))  # Increase the size of the scatter plot
ratings_count = data_cleaned.select('ratingsCount').rdd.flatMap(lambda x: x).collect()
review_score = data_cleaned.select('review/score').rdd.flatMap(lambda x: x).collect()
plt.scatter(ratings_count, review_score, alpha=0.5)
plt.title('Scatter Plot: Ratings Count vs Review Score', fontsize=15)
plt.xlabel('Ratings Count')
plt.ylabel('Review Score')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Filter books with word count greater than 1707
most_reviewed_books = data_cleaned.filter(data_cleaned['word_count'] > 1707)

# Sort the most reviewed books by word count in descending order and select the top 5
most_reviewed_books = most_reviewed_books.orderBy('word_count', ascending=False).limit(5)

# Convert Spark DataFrame to Pandas DataFrame for plotting
most_reviewed_books_pd = most_reviewed_books.toPandas()

# Plotting the pie chart
plt.figure(figsize=(10, 10))
plt.pie(most_reviewed_books_pd['word_count'], labels=most_reviewed_books_pd['Title'], autopct='%1.1f%%', startangle=140)
plt.title('Top Most Reviewed Books by Word Count', fontsize=15)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

